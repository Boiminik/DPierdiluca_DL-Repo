{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural network, math experimentation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative numbers\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# we also need a derived version of ReLu later\n",
    "# otherwise the same than original, but instead of original value\n",
    "# return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights and biases\n",
    "# in Keras etc. these are usually randomized in the beginning\n",
    "w1 = 1\n",
    "w2 = 0.5\n",
    "w3 = 1\n",
    "w4 = -0.5\n",
    "w5 = 1\n",
    "w6 = 1\n",
    "w7 = 0\n",
    "w8 = 1\n",
    "w9 = 0.5\n",
    "bias1 = 0.5\n",
    "bias2 = 0\n",
    "bias3 = 0.5\n",
    "bias4 = 0.5\n",
    "\n",
    "# our training data\n",
    "# x1 = input1, x2 = input2, y = true_value\n",
    "input1 = 1\n",
    "input2 = 0\n",
    "true_value = 2\n",
    "\n",
    "# our learning rate\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>FORWARD PASS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NODE 1 OUTPUT\n",
    "node_1_output = input1 * w1 + input2 * w4 + bias1\n",
    "node_1_output = activation_ReLu(node_1_output)\n",
    "node_1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 841,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NODE 2 OUTPUT\n",
    "node_2_output = input1 * w2 + input2 * w5 + bias2\n",
    "node_2_output = activation_ReLu(node_2_output)\n",
    "node_2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_3_output = input1 * w3 + input2 * w6 + bias3\n",
    "node_3_output = activation_ReLu(node_3_output)\n",
    "node_3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.75"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NODE 3 OUTPUT\n",
    "# we can just use Node 1 and 2 outputs, since they\n",
    "# already contain the the previous weights\n",
    "node_4_output = node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias3\n",
    "node_4_output = activation_ReLu(node_4_output)\n",
    "node_4_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1.75 --> True value: 2\n"
     ]
    }
   ],
   "source": [
    "# compare predicted value with true value\n",
    "print(f\"Predicted: {node_4_output} --> True value: {true_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOSS FUNCTION - we are going to use MSE -> mean squared error\n",
    "# MSE formula LOSS => (predicted_value - true_value) ^ 2\n",
    "predicted_value = node_4_output\n",
    "loss = (predicted_value - true_value) ** 2\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>BACKPROPAGATION - update the weights and biases while traversing the network BACKWARDS</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.75"
      ]
     },
     "execution_count": 846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to weight 5\n",
    "deriv_L_w7 = 2 * node_1_output * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0075"
      ]
     },
     "execution_count": 847,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this decreases the weight value a little bit\n",
    "# this is basically our optimizer + learning rate\n",
    "# this optimizer is known as gradient descent\n",
    "new_w7 = w7 - LR * deriv_L_w7\n",
    "new_w7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.25"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to weight 6\n",
    "deriv_L_w8 = 2 * node_2_output * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0025"
      ]
     },
     "execution_count": 849,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate new value for weight 6\n",
    "new_w8 = w8 - LR * deriv_L_w8\n",
    "new_w8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.75"
      ]
     },
     "execution_count": 850,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to weight 6\n",
    "deriv_L_w9 = 2 * node_3_output * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5075"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate new value for weight 6\n",
    "new_w9 = w9 - LR * deriv_L_w9\n",
    "new_w9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to bias3\n",
    "# NOTE: the * 1 comes from derivating the bias, which is same as derivating x, \n",
    "# which result in 1\n",
    "deriv_L_b4 = 2 * 1 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.505"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the bias 3 based on previous derivation\n",
    "new_b4 = bias4 - LR * deriv_L_b4\n",
    "new_b4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>To access the first layer, we need to use chain rule, in order to calculate new values for w1-w4 and bias1/2</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see materials for how we need to split this calculation into two parts\n",
    "# here we solve the left and right sides separately\n",
    "\n",
    "# left side is mostly the same as derivating w5 and w6\n",
    "deriv_L_w1_left = 2 * w7* (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4- true_value)\n",
    "\n",
    "# right side: use ReLu derivation and remember to match correct weights with correct inputs and biases\n",
    "# based on which weight are you are derivating\n",
    "# COMPARE THE OTHER ORIGINAL PICTURE IN THE MATERIALS\n",
    "# in the case of w1 => use w1 and and w3 inside the Relu-derivation, because\n",
    "# these weights are connected to node 1 (which is connected to w1)\n",
    "# also use bias1, since it's part of node 1\n",
    "# finally, multiply all with input1, because it is connected to w1\n",
    "deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w4 + bias1) * input1\n",
    "deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "new_w1 = w1 - LR * deriv_L_w1\n",
    "new_w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.505"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same logic as above, but now from the point of view of w2\n",
    "# notice how we use w6 and w2/w4 and bias2 in the equation of right side\n",
    "deriv_L_w2_left = 2 * w8 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w5 + bias2) * input1\n",
    "deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "new_w2 = w2 - LR * deriv_L_w2\n",
    "new_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 856,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w3_left = 2 * w9 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w3 + input2 * w6 + bias1) * input2\n",
    "deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "new_w3 = w3 - LR * deriv_L_w3\n",
    "new_w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 857,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w4_left = 2 * w7 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w4 + bias2) * input2\n",
    "deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "new_w4 = w4 - LR * deriv_L_w4\n",
    "new_w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w5_left = 2 * w8 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w5_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w5 + bias2) * input2\n",
    "deriv_L_w5 = deriv_L_w5_left * deriv_L_w5_right\n",
    "new_w5 = w5 - LR * deriv_L_w5\n",
    "new_w5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w6_left = 2 * w9 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w6_right = activation_ReLu_partial_derivative(input1 * w3 + input2 * w6 + bias2) * input2\n",
    "deriv_L_w6 = deriv_L_w6_left * deriv_L_w6_right\n",
    "new_w6 = w6 - LR * deriv_L_w6\n",
    "new_w6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# otherwise the same formula, but we can multiply the right side function with just 1\n",
    "# because it's a derivation of bias-term, which is the same as derivation of x, which results in 1\n",
    "deriv_L_b1_left = 2 * w7 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w4 + bias1) * 1\n",
    "deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "new_b1 = bias1 - LR * deriv_L_b1\n",
    "new_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar derivation of bias 2\n",
    "deriv_L_b2_left = 2 * w8 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w5 + bias2) * 1\n",
    "deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "new_b2 = bias2 - LR * deriv_L_b2\n",
    "new_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5025"
      ]
     },
     "execution_count": 862,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# otherwise the same formula, but we can multiply the right side function with just 1\n",
    "# because it's a derivation of bias-term, which is the same as derivation of x, which results in 1\n",
    "deriv_L_b3_left = 2 * w9 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b3_right = activation_ReLu_partial_derivative(input1 * w3 + input2 * w6 + bias1) * 1\n",
    "deriv_L_b3 = deriv_L_b3_left * deriv_L_b3_right\n",
    "new_b3 = bias3 - LR * deriv_L_b3\n",
    "new_b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Everything should be okay now, let's compare the results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1\n",
      "w2: 0.5\n",
      "w3: 1\n",
      "w4: -0.5\n",
      "w5: 1\n",
      "w6: 1\n",
      "w7: 0\n",
      "w8: 1\n",
      "w9: 0.5\n",
      "b1: 0.5\n",
      "b2: 0\n",
      "b3: 0.5\n",
      "b4: 0.5\n",
      "\n",
      "\n",
      "######################################\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: 1.0\n",
      "w2: 0.505\n",
      "w3: 1.0\n",
      "w4: -0.5\n",
      "w5: 1.0\n",
      "w6: 1.0\n",
      "w7: 0.0075\n",
      "w8: 1.0025\n",
      "w9: 0.5075\n",
      "b1: 0.5\n",
      "b2: 0.005\n",
      "b3: 0.5025\n",
      "b4: 0.505\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {w1}\")\n",
    "print(f\"w2: {w2}\")\n",
    "print(f\"w3: {w3}\")\n",
    "print(f\"w4: {w4}\")\n",
    "print(f\"w5: {w5}\")\n",
    "print(f\"w6: {w6}\")\n",
    "print(f\"w7: {w7}\")\n",
    "print(f\"w8: {w8}\")\n",
    "print(f\"w9: {w9}\")\n",
    "print(f\"b1: {bias1}\")\n",
    "print(f\"b2: {bias2}\")\n",
    "print(f\"b3: {bias3}\")\n",
    "print(f\"b4: {bias4}\")\n",
    "\n",
    "print(\"\\n\\n######################################\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {new_w1}\")\n",
    "print(f\"w2: {new_w2}\")\n",
    "print(f\"w3: {new_w3}\")\n",
    "print(f\"w4: {new_w4}\")\n",
    "print(f\"w5: {new_w5}\")\n",
    "print(f\"w6: {new_w6}\")\n",
    "print(f\"w7: {new_w7}\")\n",
    "print(f\"w8: {new_w8}\")\n",
    "print(f\"w9: {new_w9}\")\n",
    "print(f\"b1: {new_b1}\")\n",
    "print(f\"b2: {new_b2}\")\n",
    "print(f\"b3: {new_b3}\")\n",
    "print(f\"b4: {new_b4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Trying with new values and increased learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights and biases\n",
    "# in Keras etc. these are usually randomized in the beginning\n",
    "w1 = new_w1\n",
    "w2 = new_w2\n",
    "w3 = new_w3\n",
    "w4 = new_w4\n",
    "w5 = new_w5\n",
    "w6 = new_w6\n",
    "w7 = new_w7\n",
    "w8 = new_w8\n",
    "w9 = new_w9\n",
    "bias1 = new_b1\n",
    "bias2 = new_b2\n",
    "bias3 = new_b3\n",
    "bias4 = new_b4\n",
    "\n",
    "# our training data\n",
    "# x1 = input1, x2 = input2, y = true_value\n",
    "input1 = 1\n",
    "input2 = 0\n",
    "true_value = 2\n",
    "\n",
    "# our learning rate\n",
    "LR = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>FORWARD PASS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NODE 1 OUTPUT\n",
    "node_1_output = input1 * w1 + input2 * w4 + bias1\n",
    "node_1_output = activation_ReLu(node_1_output)\n",
    "node_1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51"
      ]
     },
     "execution_count": 866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NODE 2 OUTPUT\n",
    "node_2_output = input1 * w2 + input2 * w5 + bias2\n",
    "node_2_output = activation_ReLu(node_2_output)\n",
    "node_2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5025"
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_3_output = input1 * w3 + input2 * w6 + bias3\n",
    "node_3_output = activation_ReLu(node_3_output)\n",
    "node_3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7875437499999998"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NODE 3 OUTPUT\n",
    "# we can just use Node 1 and 2 outputs, since they\n",
    "# already contain the the previous weights\n",
    "node_4_output = node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias3\n",
    "node_4_output = activation_ReLu(node_4_output)\n",
    "node_4_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1.7875437499999998 --> True value: 2\n"
     ]
    }
   ],
   "source": [
    "# compare predicted value with true value\n",
    "print(f\"Predicted: {node_4_output} --> True value: {true_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0451376581640626"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOSS FUNCTION - we are going to use MSE -> mean squared error\n",
    "# MSE formula LOSS => (predicted_value - true_value) ^ 2\n",
    "predicted_value = node_4_output\n",
    "loss = (predicted_value - true_value) ** 2\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>BACKPROPAGATION - update the weights and biases while traversing the network BACKWARDS</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6298687500000009"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to weight 5\n",
    "deriv_L_w7 = 2 * node_1_output * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03899343750000005"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this decreases the weight value a little bit\n",
    "# this is basically our optimizer + learning rate\n",
    "# this optimizer is known as gradient descent\n",
    "new_w7 = w7 - LR * deriv_L_w7\n",
    "new_w7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2141553750000003"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to weight 6\n",
    "deriv_L_w8 = 2 * node_2_output * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.01320776875"
      ]
     },
     "execution_count": 874,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate new value for weight 6\n",
    "new_w8 = w8 - LR * deriv_L_w8\n",
    "new_w8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6309185312500009"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to weight 6\n",
    "deriv_L_w9 = 2 * node_3_output * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5390459265624999"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate new value for weight 6\n",
    "new_w9 = w9 - LR * deriv_L_w9\n",
    "new_w9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4199125000000006"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to bias3\n",
    "# NOTE: the * 1 comes from derivating the bias, which is same as derivating x, \n",
    "# which result in 1\n",
    "deriv_L_b4 = 2 * 1 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5259956250000001"
      ]
     },
     "execution_count": 878,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the bias 3 based on previous derivation\n",
    "new_b4 = bias4 - LR * deriv_L_b4\n",
    "new_b4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>To access the first layer, we need to use chain rule, in order to calculate new values for w1-w4 and bias1/2</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0001574671875"
      ]
     },
     "execution_count": 879,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see materials for how we need to split this calculation into two parts\n",
    "# here we solve the left and right sides separately\n",
    "\n",
    "# left side is mostly the same as derivating w5 and w6\n",
    "deriv_L_w1_left = 2 * w7* (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4- true_value)\n",
    "\n",
    "# right side: use ReLu derivation and remember to match correct weights with correct inputs and biases\n",
    "# based on which weight are you are derivating\n",
    "# COMPARE THE OTHER ORIGINAL PICTURE IN THE MATERIALS\n",
    "# in the case of w1 => use w1 and and w3 inside the Relu-derivation, because\n",
    "# these weights are connected to node 1 (which is connected to w1)\n",
    "# also use bias1, since it's part of node 1\n",
    "# finally, multiply all with input1, because it is connected to w1\n",
    "deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w4 + bias1) * input1\n",
    "deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "new_w1 = w1 - LR * deriv_L_w1\n",
    "new_w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5260481140625001"
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same logic as above, but now from the point of view of w2\n",
    "# notice how we use w6 and w2/w4 and bias2 in the equation of right side\n",
    "deriv_L_w2_left = 2 * w8 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w5 + bias2) * input1\n",
    "deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "new_w2 = w2 - LR * deriv_L_w2\n",
    "new_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w3_left = 2 * w9 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w3 + input2 * w6 + bias1) * input2\n",
    "deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "new_w3 = w3 - LR * deriv_L_w3\n",
    "new_w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w4_left = 2 * w7 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w4 + bias2) * input2\n",
    "deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "new_w4 = w4 - LR * deriv_L_w4\n",
    "new_w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 883,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w5_left = 2 * w8 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w5_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w5 + bias2) * input2\n",
    "deriv_L_w5 = deriv_L_w5_left * deriv_L_w5_right\n",
    "new_w5 = w5 - LR * deriv_L_w5\n",
    "new_w5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w6_left = 2 * w9 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w6_right = activation_ReLu_partial_derivative(input1 * w3 + input2 * w6 + bias2) * input2\n",
    "deriv_L_w6 = deriv_L_w6_left * deriv_L_w6_right\n",
    "new_w6 = w6 - LR * deriv_L_w6\n",
    "new_w6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5001574671875"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# otherwise the same formula, but we can multiply the right side function with just 1\n",
    "# because it's a derivation of bias-term, which is the same as derivation of x, which results in 1\n",
    "deriv_L_b1_left = 2 * w7 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w4 + bias1) * 1\n",
    "deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "new_b1 = bias1 - LR * deriv_L_b1\n",
    "new_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02604811406250003"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar derivation of bias 2\n",
    "deriv_L_b2_left = 2 * w8 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w5 + bias2) * 1\n",
    "deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "new_b2 = bias2 - LR * deriv_L_b2\n",
    "new_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5131552796875"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# otherwise the same formula, but we can multiply the right side function with just 1\n",
    "# because it's a derivation of bias-term, which is the same as derivation of x, which results in 1\n",
    "deriv_L_b3_left = 2 * w9 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b3_right = activation_ReLu_partial_derivative(input1 * w3 + input2 * w6 + bias1) * 1\n",
    "deriv_L_b3 = deriv_L_b3_left * deriv_L_b3_right\n",
    "new_b3 = bias3 - LR * deriv_L_b3\n",
    "new_b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Everything should be okay now, let's compare the results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1.0\n",
      "w2: 0.505\n",
      "w3: 1.0\n",
      "w4: -0.5\n",
      "w5: 1.0\n",
      "w6: 1.0\n",
      "w7: 0.0075\n",
      "w8: 1.0025\n",
      "w9: 0.5075\n",
      "b1: 0.5\n",
      "b2: 0.005\n",
      "b3: 0.5025\n",
      "b4: 0.505\n",
      "\n",
      "\n",
      "######################################\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: 1.0001574671875\n",
      "w2: 0.5260481140625001\n",
      "w3: 1.0\n",
      "w4: -0.5\n",
      "w5: 1.0\n",
      "w6: 1.0\n",
      "w7: 0.03899343750000005\n",
      "w8: 1.01320776875\n",
      "w9: 0.5390459265624999\n",
      "b1: 0.5001574671875\n",
      "b2: 0.02604811406250003\n",
      "b3: 0.5131552796875\n",
      "b4: 0.5259956250000001\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {w1}\")\n",
    "print(f\"w2: {w2}\")\n",
    "print(f\"w3: {w3}\")\n",
    "print(f\"w4: {w4}\")\n",
    "print(f\"w5: {w5}\")\n",
    "print(f\"w6: {w6}\")\n",
    "print(f\"w7: {w7}\")\n",
    "print(f\"w8: {w8}\")\n",
    "print(f\"w9: {w9}\")\n",
    "print(f\"b1: {bias1}\")\n",
    "print(f\"b2: {bias2}\")\n",
    "print(f\"b3: {bias3}\")\n",
    "print(f\"b4: {bias4}\")\n",
    "\n",
    "print(\"\\n\\n######################################\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {new_w1}\")\n",
    "print(f\"w2: {new_w2}\")\n",
    "print(f\"w3: {new_w3}\")\n",
    "print(f\"w4: {new_w4}\")\n",
    "print(f\"w5: {new_w5}\")\n",
    "print(f\"w6: {new_w6}\")\n",
    "print(f\"w7: {new_w7}\")\n",
    "print(f\"w8: {new_w8}\")\n",
    "print(f\"w9: {new_w9}\")\n",
    "print(f\"b1: {new_b1}\")\n",
    "print(f\"b2: {new_b2}\")\n",
    "print(f\"b3: {new_b3}\")\n",
    "print(f\"b4: {new_b4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Trying with new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights and biases\n",
    "# in Keras etc. these are usually randomized in the beginning\n",
    "w1 = new_w1\n",
    "w2 = new_w2\n",
    "w3 = new_w3\n",
    "w4 = new_w4\n",
    "w5 = new_w5\n",
    "w6 = new_w6\n",
    "w7 = new_w7\n",
    "w8 = new_w8\n",
    "w9 = new_w9\n",
    "bias1 = new_b1\n",
    "bias2 = new_b2\n",
    "bias3 = new_b3\n",
    "bias4 = new_b4\n",
    "\n",
    "# our training data\n",
    "# x1 = input1, x2 = input2, y = true_value\n",
    "input1 = 1\n",
    "input2 = 0\n",
    "true_value = 2\n",
    "\n",
    "# our learning rate\n",
    "LR = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>FORWARD PASS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.500314934375"
      ]
     },
     "execution_count": 890,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NODE 1 OUTPUT\n",
    "node_1_output = input1 * w1 + input2 * w4 + bias1\n",
    "node_1_output = activation_ReLu(node_1_output)\n",
    "node_1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5520962281250001"
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NODE 2 OUTPUT\n",
    "node_2_output = input1 * w2 + input2 * w5 + bias2\n",
    "node_2_output = activation_ReLu(node_2_output)\n",
    "node_2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5131552796875"
      ]
     },
     "execution_count": 892,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_3_output = input1 * w3 + input2 * w6 + bias3\n",
    "node_3_output = activation_ReLu(node_3_output)\n",
    "node_3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9467060935172777"
      ]
     },
     "execution_count": 893,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NODE 3 OUTPUT\n",
    "# we can just use Node 1 and 2 outputs, since they\n",
    "# already contain the the previous weights\n",
    "node_4_output = node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias3\n",
    "node_4_output = activation_ReLu(node_4_output)\n",
    "node_4_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1.9467060935172777 --> True value: 2\n"
     ]
    }
   ],
   "source": [
    "# compare predicted value with true value\n",
    "print(f\"Predicted: {node_4_output} --> True value: {true_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002840240468189145"
      ]
     },
     "execution_count": 895,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOSS FUNCTION - we are going to use MSE -> mean squared error\n",
    "# MSE formula LOSS => (predicted_value - true_value) ^ 2\n",
    "predicted_value = node_4_output\n",
    "loss = (predicted_value - true_value) ** 2\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>BACKPROPAGATION - update the weights and biases while traversing the network BACKWARDS</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12138616394467376"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to weight 5\n",
    "deriv_L_w7 = 2 * node_1_output * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05113205389446743"
      ]
     },
     "execution_count": 897,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this decreases the weight value a little bit\n",
    "# this is basically our optimizer + learning rate\n",
    "# this optimizer is known as gradient descent\n",
    "new_w7 = w7 - LR * deriv_L_w7\n",
    "new_w7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.044668517072607224"
      ]
     },
     "execution_count": 898,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to weight 6\n",
    "deriv_L_w8 = 2 * node_2_output * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0176746204572609"
      ]
     },
     "execution_count": 899,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate new value for weight 6\n",
    "new_w8 = w8 - LR * deriv_L_w8\n",
    "new_w8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12242503933376575"
      ]
     },
     "execution_count": 900,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to weight 6\n",
    "deriv_L_w9 = 2 * node_3_output * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5512884304958765"
      ]
     },
     "execution_count": 901,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate new value for weight 6\n",
    "new_w9 = w9 - LR * deriv_L_w9\n",
    "new_w9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08090712234044428"
      ]
     },
     "execution_count": 902,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to bias3\n",
    "# NOTE: the * 1 comes from derivating the bias, which is same as derivating x, \n",
    "# which result in 1\n",
    "deriv_L_b4 = 2 * 1 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5340863372340445"
      ]
     },
     "execution_count": 903,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the bias 3 based on previous derivation\n",
    "new_b4 = bias4 - LR * deriv_L_b4\n",
    "new_b4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>To access the first layer, we need to use chain rule, in order to calculate new values for w1-w4 and bias1/2</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0004729518693287"
      ]
     },
     "execution_count": 904,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see materials for how we need to split this calculation into two parts\n",
    "# here we solve the left and right sides separately\n",
    "\n",
    "# left side is mostly the same as derivating w5 and w6\n",
    "deriv_L_w1_left = 2 * w7* (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4- true_value)\n",
    "\n",
    "# right side: use ReLu derivation and remember to match correct weights with correct inputs and biases\n",
    "# based on which weight are you are derivating\n",
    "# COMPARE THE OTHER ORIGINAL PICTURE IN THE MATERIALS\n",
    "# in the case of w1 => use w1 and and w3 inside the Relu-derivation, because\n",
    "# these weights are connected to node 1 (which is connected to w1)\n",
    "# also use bias1, since it's part of node 1\n",
    "# finally, multiply all with input1, because it is connected to w1\n",
    "deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w4 + bias1) * input1\n",
    "deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "new_w1 = w1 - LR * deriv_L_w1\n",
    "new_w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5342456865527545"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same logic as above, but now from the point of view of w2\n",
    "# notice how we use w6 and w2/w4 and bias2 in the equation of right side\n",
    "deriv_L_w2_left = 2 * w8 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w5 + bias2) * input1\n",
    "deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "new_w2 = w2 - LR * deriv_L_w2\n",
    "new_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w3_left = 2 * w9 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w3 + input2 * w6 + bias1) * input2\n",
    "deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "new_w3 = w3 - LR * deriv_L_w3\n",
    "new_w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 907,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w4_left = 2 * w7 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w4 + bias2) * input2\n",
    "deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "new_w4 = w4 - LR * deriv_L_w4\n",
    "new_w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 908,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w5_left = 2 * w8 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w5_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w5 + bias2) * input2\n",
    "deriv_L_w5 = deriv_L_w5_left * deriv_L_w5_right\n",
    "new_w5 = w5 - LR * deriv_L_w5\n",
    "new_w5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 909,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w6_left = 2 * w9 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w6_right = activation_ReLu_partial_derivative(input1 * w3 + input2 * w6 + bias2) * input2\n",
    "deriv_L_w6 = deriv_L_w6_left * deriv_L_w6_right\n",
    "new_w6 = w6 - LR * deriv_L_w6\n",
    "new_w6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5004729518693287"
      ]
     },
     "execution_count": 910,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# otherwise the same formula, but we can multiply the right side function with just 1\n",
    "# because it's a derivation of bias-term, which is the same as derivation of x, which results in 1\n",
    "deriv_L_b1_left = 2 * w7 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w4 + bias1) * 1\n",
    "deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "new_b1 = bias1 - LR * deriv_L_b1\n",
    "new_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03424568655275451"
      ]
     },
     "execution_count": 911,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar derivation of bias 2\n",
    "deriv_L_b2_left = 2 * w8 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w5 + bias2) * 1\n",
    "deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "new_b2 = bias2 - LR * deriv_L_b2\n",
    "new_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.517516545160251"
      ]
     },
     "execution_count": 912,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# otherwise the same formula, but we can multiply the right side function with just 1\n",
    "# because it's a derivation of bias-term, which is the same as derivation of x, which results in 1\n",
    "deriv_L_b3_left = 2 * w9 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b3_right = activation_ReLu_partial_derivative(input1 * w3 + input2 * w6 + bias1) * 1\n",
    "deriv_L_b3 = deriv_L_b3_left * deriv_L_b3_right\n",
    "new_b3 = bias3 - LR * deriv_L_b3\n",
    "new_b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Everything should be okay now, let's compare the results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1.0001574671875\n",
      "w2: 0.5260481140625001\n",
      "w3: 1.0\n",
      "w4: -0.5\n",
      "w5: 1.0\n",
      "w6: 1.0\n",
      "w7: 0.03899343750000005\n",
      "w8: 1.01320776875\n",
      "w9: 0.5390459265624999\n",
      "b1: 0.5001574671875\n",
      "b2: 0.02604811406250003\n",
      "b3: 0.5131552796875\n",
      "b4: 0.5259956250000001\n",
      "\n",
      "\n",
      "######################################\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: 1.0004729518693287\n",
      "w2: 0.5342456865527545\n",
      "w3: 1.0\n",
      "w4: -0.5\n",
      "w5: 1.0\n",
      "w6: 1.0\n",
      "w7: 0.05113205389446743\n",
      "w8: 1.0176746204572609\n",
      "w9: 0.5512884304958765\n",
      "b1: 0.5004729518693287\n",
      "b2: 0.03424568655275451\n",
      "b3: 0.517516545160251\n",
      "b4: 0.5340863372340445\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {w1}\")\n",
    "print(f\"w2: {w2}\")\n",
    "print(f\"w3: {w3}\")\n",
    "print(f\"w4: {w4}\")\n",
    "print(f\"w5: {w5}\")\n",
    "print(f\"w6: {w6}\")\n",
    "print(f\"w7: {w7}\")\n",
    "print(f\"w8: {w8}\")\n",
    "print(f\"w9: {w9}\")\n",
    "print(f\"b1: {bias1}\")\n",
    "print(f\"b2: {bias2}\")\n",
    "print(f\"b3: {bias3}\")\n",
    "print(f\"b4: {bias4}\")\n",
    "\n",
    "print(\"\\n\\n######################################\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {new_w1}\")\n",
    "print(f\"w2: {new_w2}\")\n",
    "print(f\"w3: {new_w3}\")\n",
    "print(f\"w4: {new_w4}\")\n",
    "print(f\"w5: {new_w5}\")\n",
    "print(f\"w6: {new_w6}\")\n",
    "print(f\"w7: {new_w7}\")\n",
    "print(f\"w8: {new_w8}\")\n",
    "print(f\"w9: {new_w9}\")\n",
    "print(f\"b1: {new_b1}\")\n",
    "print(f\"b2: {new_b2}\")\n",
    "print(f\"b3: {new_b3}\")\n",
    "print(f\"b4: {new_b4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Trying with new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights and biases\n",
    "# in Keras etc. these are usually randomized in the beginning\n",
    "w1 = new_w1\n",
    "w2 = new_w2\n",
    "w3 = new_w3\n",
    "w4 = new_w4\n",
    "w5 = new_w5\n",
    "w6 = new_w6\n",
    "w7 = new_w7\n",
    "w8 = new_w8\n",
    "w9 = new_w9\n",
    "bias1 = new_b1\n",
    "bias2 = new_b2\n",
    "bias3 = new_b3\n",
    "bias4 = new_b4\n",
    "\n",
    "# our training data\n",
    "# x1 = input1, x2 = input2, y = true_value\n",
    "input1 = 1\n",
    "input2 = 0\n",
    "true_value = 2\n",
    "\n",
    "# our learning rate\n",
    "LR = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>FORWARD PASS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5009459037386574"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NODE 1 OUTPUT\n",
    "node_1_output = input1 * w1 + input2 * w4 + bias1\n",
    "node_1_output = activation_ReLu(node_1_output)\n",
    "node_1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.568491373105509"
      ]
     },
     "execution_count": 916,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NODE 2 OUTPUT\n",
    "node_2_output = input1 * w2 + input2 * w5 + bias2\n",
    "node_2_output = activation_ReLu(node_2_output)\n",
    "node_2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.517516545160251"
      ]
     },
     "execution_count": 917,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_3_output = input1 * w3 + input2 * w6 + bias3\n",
    "node_3_output = activation_ReLu(node_3_output)\n",
    "node_3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0093915487941922"
      ]
     },
     "execution_count": 918,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NODE 3 OUTPUT\n",
    "# we can just use Node 1 and 2 outputs, since they\n",
    "# already contain the the previous weights\n",
    "node_4_output = node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias3\n",
    "node_4_output = activation_ReLu(node_4_output)\n",
    "node_4_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 2.0093915487941922 --> True value: 2\n"
     ]
    }
   ],
   "source": [
    "# compare predicted value with true value\n",
    "print(f\"Predicted: {node_4_output} --> True value: {true_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.820118875369371e-05"
      ]
     },
     "execution_count": 920,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOSS FUNCTION - we are going to use MSE -> mean squared error\n",
    "# MSE formula LOSS => (predicted_value - true_value) ^ 2\n",
    "predicted_value = node_4_output\n",
    "loss = (predicted_value - true_value) ** 2\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>BACKPROPAGATION - update the weights and biases while traversing the network BACKWARDS</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07793313646273173"
      ]
     },
     "execution_count": 921,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to weight 5\n",
    "deriv_L_w7 = 2 * node_1_output * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04333874024819426"
      ]
     },
     "execution_count": 922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this decreases the weight value a little bit\n",
    "# this is basically our optimizer + learning rate\n",
    "# this optimizer is known as gradient descent\n",
    "new_w7 = w7 - LR * deriv_L_w7\n",
    "new_w7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029517596635402505"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to weight 6\n",
    "deriv_L_w8 = 2 * node_2_output * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0147228607937206"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate new value for weight 6\n",
    "new_w8 = w8 - LR * deriv_L_w8\n",
    "new_w8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07879352860342603"
      ]
     },
     "execution_count": 925,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to weight 6\n",
    "deriv_L_w9 = 2 * node_3_output * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5434090776355339"
      ]
     },
     "execution_count": 926,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate new value for weight 6\n",
    "new_w9 = w9 - LR * deriv_L_w9\n",
    "new_w9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05192268173597103"
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solving the partial derivative of the loss function with respect to bias3\n",
    "# NOTE: the * 1 comes from derivating the bias, which is same as derivating x, \n",
    "# which result in 1\n",
    "deriv_L_b4 = 2 * 1 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5288940690604474"
      ]
     },
     "execution_count": 928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the bias 3 based on previous derivation\n",
    "new_b4 = bias4 - LR * deriv_L_b4\n",
    "new_b4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>To access the first layer, we need to use chain rule, in order to calculate new values for w1-w4 and bias1/2</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0002074605332418"
      ]
     },
     "execution_count": 929,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see materials for how we need to split this calculation into two parts\n",
    "# here we solve the left and right sides separately\n",
    "\n",
    "# left side is mostly the same as derivating w5 and w6\n",
    "deriv_L_w1_left = 2 * w7* (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4- true_value)\n",
    "\n",
    "# right side: use ReLu derivation and remember to match correct weights with correct inputs and biases\n",
    "# based on which weight are you are derivating\n",
    "# COMPARE THE OTHER ORIGINAL PICTURE IN THE MATERIALS\n",
    "# in the case of w1 => use w1 and and w3 inside the Relu-derivation, because\n",
    "# these weights are connected to node 1 (which is connected to w1)\n",
    "# also use bias1, since it's part of node 1\n",
    "# finally, multiply all with input1, because it is connected to w1\n",
    "deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w4 + bias1) * input1\n",
    "deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "new_w1 = w1 - LR * deriv_L_w1\n",
    "new_w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5289616470098768"
      ]
     },
     "execution_count": 930,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same logic as above, but now from the point of view of w2\n",
    "# notice how we use w6 and w2/w4 and bias2 in the equation of right side\n",
    "deriv_L_w2_left = 2 * w8 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w5 + bias2) * input1\n",
    "deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "new_w2 = w2 - LR * deriv_L_w2\n",
    "new_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 931,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w3_left = 2 * w9 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w3 + input2 * w6 + bias1) * input2\n",
    "deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "new_w3 = w3 - LR * deriv_L_w3\n",
    "new_w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 932,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w4_left = 2 * w7 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w4 + bias2) * input2\n",
    "deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "new_w4 = w4 - LR * deriv_L_w4\n",
    "new_w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 933,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w5_left = 2 * w8 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w5_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w5 + bias2) * input2\n",
    "deriv_L_w5 = deriv_L_w5_left * deriv_L_w5_right\n",
    "new_w5 = w5 - LR * deriv_L_w5\n",
    "new_w5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 934,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same formula again\n",
    "deriv_L_w6_left = 2 * w9 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_w6_right = activation_ReLu_partial_derivative(input1 * w3 + input2 * w6 + bias2) * input2\n",
    "deriv_L_w6 = deriv_L_w6_left * deriv_L_w6_right\n",
    "new_w6 = w6 - LR * deriv_L_w6\n",
    "new_w6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5002074605332418"
      ]
     },
     "execution_count": 935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# otherwise the same formula, but we can multiply the right side function with just 1\n",
    "# because it's a derivation of bias-term, which is the same as derivation of x, which results in 1\n",
    "deriv_L_b1_left = 2 * w7 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w4 + bias1) * 1\n",
    "deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "new_b1 = bias1 - LR * deriv_L_b1\n",
    "new_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028961647009876766"
      ]
     },
     "execution_count": 936,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar derivation of bias 2\n",
    "deriv_L_b2_left = 2 * w8 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w5 + bias2) * 1\n",
    "deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "new_b2 = bias2 - LR * deriv_L_b2\n",
    "new_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.514654107788115"
      ]
     },
     "execution_count": 937,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# otherwise the same formula, but we can multiply the right side function with just 1\n",
    "# because it's a derivation of bias-term, which is the same as derivation of x, which results in 1\n",
    "deriv_L_b3_left = 2 * w9 * (node_1_output * w7 + node_2_output * w8 + node_3_output * w9 + bias4 - true_value)\n",
    "deriv_L_b3_right = activation_ReLu_partial_derivative(input1 * w3 + input2 * w6 + bias1) * 1\n",
    "deriv_L_b3 = deriv_L_b3_left * deriv_L_b3_right\n",
    "new_b3 = bias3 - LR * deriv_L_b3\n",
    "new_b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Everything should be okay now, let's compare the results</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1.0004729518693287\n",
      "w2: 0.5342456865527545\n",
      "w3: 1.0\n",
      "w4: -0.5\n",
      "w5: 1.0\n",
      "w6: 1.0\n",
      "w7: 0.05113205389446743\n",
      "w8: 1.0176746204572609\n",
      "w9: 0.5512884304958765\n",
      "b1: 0.5004729518693287\n",
      "b2: 0.03424568655275451\n",
      "b3: 0.517516545160251\n",
      "b4: 0.5340863372340445\n",
      "\n",
      "\n",
      "######################################\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: 1.0002074605332418\n",
      "w2: 0.5289616470098768\n",
      "w3: 1.0\n",
      "w4: -0.5\n",
      "w5: 1.0\n",
      "w6: 1.0\n",
      "w7: 0.04333874024819426\n",
      "w8: 1.0147228607937206\n",
      "w9: 0.5434090776355339\n",
      "b1: 0.5002074605332418\n",
      "b2: 0.028961647009876766\n",
      "b3: 0.514654107788115\n",
      "b4: 0.5288940690604474\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {w1}\")\n",
    "print(f\"w2: {w2}\")\n",
    "print(f\"w3: {w3}\")\n",
    "print(f\"w4: {w4}\")\n",
    "print(f\"w5: {w5}\")\n",
    "print(f\"w6: {w6}\")\n",
    "print(f\"w7: {w7}\")\n",
    "print(f\"w8: {w8}\")\n",
    "print(f\"w9: {w9}\")\n",
    "print(f\"b1: {bias1}\")\n",
    "print(f\"b2: {bias2}\")\n",
    "print(f\"b3: {bias3}\")\n",
    "print(f\"b4: {bias4}\")\n",
    "\n",
    "print(\"\\n\\n######################################\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {new_w1}\")\n",
    "print(f\"w2: {new_w2}\")\n",
    "print(f\"w3: {new_w3}\")\n",
    "print(f\"w4: {new_w4}\")\n",
    "print(f\"w5: {new_w5}\")\n",
    "print(f\"w6: {new_w6}\")\n",
    "print(f\"w7: {new_w7}\")\n",
    "print(f\"w8: {new_w8}\")\n",
    "print(f\"w9: {new_w9}\")\n",
    "print(f\"b1: {new_b1}\")\n",
    "print(f\"b2: {new_b2}\")\n",
    "print(f\"b3: {new_b3}\")\n",
    "print(f\"b4: {new_b4}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
